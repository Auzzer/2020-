##############   计算样本距离
a1=c(6901.6,2321.3,4632.8,1558.2,3447.0,3018.5,2313.6,802.8)
a2=c(8467.3,1903.9,7385.4,1420.7,5100.9,3452.3,1691.9,645.3)
a3=c(5067.7,1746.6,3753.4,1430.2,1993.8,2078.8,1524.5,492.8)
a4=c(5777.3,1776.9,3752.6,1329.1,2517.9,2322.1,1583.4,479.9)
a5=c(5975.7,1963.5,3809.4,1322.1,3064.3,2352.9,1750.4,614.9)
X=rbind(a1,a2,a3,a4,a5)
D=matrix(NA,nrow=5,ncol=5)
for (i in 1:5){
  for(j in 1:5)
  {
    D[i,j]=sqrt(t(X[i,]-X[j,])%*%(X[i,]-X[j,]))
  }
}
D
#######################



#假定数据存储目录为“E:/laimsa/”
setwd("E:/laimsa")

data1<-read.table("E:/laimsa/clus1.csv", header=T, row.names="region", sep=",") #读取文本文件
d<-dist(scale(data1), method="euclidean", diag=T, upper=F, p=2) #method为距离计算方法，缺省时为"euclidean"（欧氏距离），还包括："manhattan"（绝对值距离），"minkowski"（明氏距离），"canberra"（兰氏距离）等
#diag为是否包括对角线元素（缺省时为F），upper为是否包括上三角距离（缺省时为F），p为明氏距离的幂（p=2即为欧氏距离）
hc<-hclust(d, "ward.D") #离差平方和法
#方法还包括："single"（最短距离法），"complete"（最长距离法），"average"（类平均法），"centroid"（重心法），"median"（中间距离法）等
cbind(hc$merge, round(hc$height,2)) #聚类过程
plot(hc, hang=-1) #聚类树形图，hang指定标签在图形中所处的高度（负值时挂在0下面）
rect.hclust(hc, k=3) #将聚成的三类用边框界定
cutree(hc, k=3) #将聚成三类的结果分别以1, 2, 3表示


data2<-read.table("E:/laimsa/clus2.csv", header=T, sep=",") #读取文本文件
d<-as.dist(1-data2[-1], diag=T) #转换为距离矩阵
d
hc<-hclust(d, "complete") #最长距离法
plot(hc, hang=-1) #树形图
rect.hclust(hc, k=2) #将聚成的两类用边框界定
cutree(hc, k=2) #将聚成两类的结果分别以1, 2表示



Y=read.csv("E:/laimsa/fendiq2013.csv");y=Y[,-1]
rangy=apply(y,2,max)-apply(y,2,min) #为做0-1极差标准化
y=(y-rep(apply(y,2,min),each=nrow(y)))/rep(rangy,each=nrow(y))
hh=hclust(dist(y)^2,"complete") #平方欧氏距离和类间最长距离
plot(hh,labels=Y[,1],cex=.6)

set.seed(1010)
a=kmeans(Y[,-1],4);id=a$cluster; #k均值法
list(Y[(id==1),1],Y[(id==2),1],Y[(id==3),1],Y[(id==4),1])

library(cluster)
pamx <- pam(Y[,-1],4);id=summary(pamx)$id #最终聚类种子 k中心点聚类法
cl=summary(pamx)$clustering
list(Y[(cl==1),1],Y[(cl==2),1],Y[(cl==3),1],Y[(cl==4),1],Y[id,1]) # 最后一行输出各类最中心的样品点

plot(silhouette(pam(Y[,-1],2)))
plot(silhouette(pam(Y[,-1],3)))
plot(silhouette(pam(Y[,-1],4)))
plot(silhouette(pam(Y[,-1],5)))  ### Silhouette方法确定聚类个数，该值越大越好，》0.5表示分类效果好，《0.2表示分类效果不好




data3<-read.table("E:/laimsa/clus1.csv", header=T, row.names="region", sep=",") #读取文本文件
km<-kmeans(scale(data3), 3) #k均值法，聚成3类
sort(km$cluster) #对聚类结果进行排序

data4<-read.table("E:/laimsa/clus1.csv", header=T, row.names="region", sep=",") #读取文本文件
KM<-kmeans(data4,3,nstart=20,algorithm="Hartigan-Wong") #聚类的个数为4, 随机集合#的个数为20, 算法为"Hartigan-Wong", 其他备选算法为"Lloyd","Forgy", "MacQueen" 算法不同，得到的聚类结果也可能不同
sort(KM$cluster) #对分类结果进行排序并查看分类情况


#################### 确定类别的个数

library(NbClust)
#  Selecting the number of clusters

data1.scaled <- scale(data1)
nc <- NbClust(data1.scaled, distance="euclidean", 
              min.nc=2, max.nc=15, method="average")
par(opar)
table(nc$Best.n[1,])
barplot(table(nc$Best.n[1,]), 
        xlab="Numer of Clusters", ylab="Number of Criteria",
        main="Number of Clusters Chosen by 26 Criteria") 
#################################################################




########  模糊聚类法
library(foreign)
mydata<-read.spss("E:/laimsa/3.7.1Asia.sav") 
#读取E:/laimsa下名为3.7.1Asia.sav的SPSS原始数据
X<-as.data.frame(mydata)  #转换数据格式
Z<-data.frame(scale(X[,2:7]),row.names=X[,1]) #对数据进行标准化，并将各行命名为相应的国家
library(cluster)
fresult<-fanny(Z,3)
summary(fresult)
plot(fresult)  
## Silhouette方法确定聚类个数，该值越大越好，》0.5表示分类效果好，《0.2表示分类效果不好
##########################





